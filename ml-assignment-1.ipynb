{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T10:29:37.316222Z","iopub.execute_input":"2025-09-05T10:29:37.316831Z","iopub.status.idle":"2025-09-05T10:29:37.322026Z","shell.execute_reply.started":"2025-09-05T10:29:37.316804Z","shell.execute_reply":"2025-09-05T10:29:37.320946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q1) a:Load the data into memory. Make an appropriate X matrix and y vector.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndata = load_breast_cancer()\nprint(data.target_names)\nX = data.data.astype(float) #Extract Training data from loaded_dataset and convert into appropriate format \ny = data.target\nscaler = StandardScaler()\nX = scaler.fit_transform(X) #Fit and transform the data \nprint(X.shape, y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:36:06.358218Z","iopub.execute_input":"2025-09-05T11:36:06.358548Z","iopub.status.idle":"2025-09-05T11:36:06.521734Z","shell.execute_reply.started":"2025-09-05T11:36:06.358523Z","shell.execute_reply":"2025-09-05T11:36:06.520540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"b:Split the data at random into one set (Xtrain, ytrain) containing 80% of the instances, which will\nbe used for training + validation, and a testing set Xtest, ytest) (containing remaining instances).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y) #It tells train_test_split to split the data so that the class proportions in y are preserved in both the training and testing sets.\n\nprint(X_train.shape, X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:36:09.517884Z","iopub.execute_input":"2025-09-05T11:36:09.518228Z","iopub.status.idle":"2025-09-05T11:36:09.537549Z","shell.execute_reply.started":"2025-09-05T11:36:09.518207Z","shell.execute_reply":"2025-09-05T11:36:09.536247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"c:Give the objective of logistic regression with L2 regularization.","metadata":{}},{"cell_type":"markdown","source":"We use L2 regularization in logistic regression to prevent from overfitting.\nWithout regularization\nLogistic regression just tries to fit the training data as well as possible.\nIf our dataset has many features the model might assign very large weights to some noisy features to perfectly fit the training data.\nThis can lead to overfitting the model works great on training data but fails on new test data.So to avoid this we are using L2 regularisation which adds penality for large weights and it keeps the weights balanced and small and prevents over fitting .","metadata":{}},{"cell_type":"markdown","source":"d:Run logistic regression on the data using L2 regularization, varying the regularization parameter\nλ ∈ {0, 0.1, 1, 10, 100, 1000}. Plot on one graph the average cross-entropy for the training data\nand the testing data (averaged over all instances), as a function of λ (you should use a log scale\nfor λ). Plot on another graph the L2 norm of the weight vector you obtain. Plot on the third\ngraph the actual values of the weights obtained (one curve per weight). Finally, plot on a\ngraph the accuracy on the training and test set. Explain briefly what you see.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nimport matplotlib.pyplot as plt\n\nlambdas = [0, 0.1, 1, 10, 100, 1000]\n\ntrain_losses = []\ntest_losses = []\nweight_norms = []\nall_weights = []\ntrain_accs = []\ntest_accs = []\n\nfor lam in lambdas:\n    if lam == 0:\n        model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=5000)\n    else:\n        model = LogisticRegression(penalty='l2', C=1/lam, solver='lbfgs', max_iter=5000)\n    \n    model.fit(X_train, y_train)\n\n    # Predictions\n    y_train_prob = model.predict_proba(X_train)\n    y_test_prob = model.predict_proba(X_test)\n\n    # Cross entropy loss\n    train_losses.append(log_loss(y_train, y_train_prob))\n    test_losses.append(log_loss(y_test, y_test_prob))\n\n    # L2 norm of weights\n    weight_norms.append(np.linalg.norm(model.coef_))\n\n    # Store all weights\n    all_weights.append(model.coef_.flatten())\n\n    # Accuracy\n    train_accs.append(accuracy_score(y_train, model.predict(X_train)))\n    test_accs.append(accuracy_score(y_test, model.predict(X_test)))\n\n# Cross-entropy vs lambda\nplt.figure(figsize=(10,6))\nplt.plot(lambdas, train_losses, marker='o', label='Train Loss')\nplt.plot(lambdas, test_losses, marker='s', label='Test Loss')\nplt.xscale('log')\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Cross-Entropy Loss vs Regularization Strength\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Weight Norm vs lambda\nplt.figure(figsize=(10,6))\nplt.plot(lambdas, weight_norms, marker='o')\nplt.xscale('log')\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"L2 Norm of Weights\")\nplt.title(\"L2 Norm of Weight Vector vs Regularization Strength\")\nplt.grid(True)\nplt.show()\n\n# Weights vs lambda\nall_weights=np.array(all_weights)\nplt.figure(figsize=(12,7))\nfor j in range(all_weights.shape[1]):\n    plt.plot(lambdas, all_weights[:,j], label=f'Weight {j}')\nplt.xscale('log')\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Weight Values\")\nplt.title(\"Individual Weights vs Regularization Strength\")\nplt.grid(True)\nplt.show()\n\n# Accuracy vs lambda\nplt.figure(figsize=(10,6))\nplt.plot(lambdas, train_accs, marker='o', label='Train Accuracy')\nplt.plot(lambdas, test_accs, marker='s', label='Test Accuracy')\nplt.xscale('log')\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Train/Test Accuracy vs Regularization Strength\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:36:12.442539Z","iopub.execute_input":"2025-09-05T11:36:12.442963Z","iopub.status.idle":"2025-09-05T11:36:15.536312Z","shell.execute_reply.started":"2025-09-05T11:36:12.442914Z","shell.execute_reply":"2025-09-05T11:36:15.534982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"e:Re-format the data in the following way: take each of the input variables, and feed it through\na set of Gaussian basis functions, defined as follows. For each variable (except the bias term),\nuse 5 univariate basis functions with means evenly spaced between -10 and 10 and variance σ.\nYou will experiment with σ values of 0.1, 0.5, 1, 5 and 10.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef gbt(X, sigma, n_basis=5, low=-10, high=10, add_bias=True):\n    \"\"\"\n    Gaussian Basis Transform\n    \n    X: Input matrix of shape (n_samples, n_features)\n    sigma: Standard deviation of Gaussian\n    n_basis: Number of Gaussian basis functions per feature\n    low, high: Range for evenly spaced means\n    add_bias: Whether to add bias (constant 1 column)\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    # Means for Gaussian centers\n    means = np.linspace(low, high, n_basis)\n    print(\"Equally spaced means:\", means)\n    \n    # Store transformed features\n    transformed = []\n    \n    for j in range(n_features):\n        for m in means:\n            # Gaussian basis: exp(- (x - m)^2 / (2σ^2))\n            transformed.append(np.exp(- (X[:, j] - m) ** 2 / (2 * sigma ** 2)))\n    \n    # Stack into shape (n_samples, n_features * n_basis)\n    X_new = np.array(transformed).T  \n    \n    # Add bias term if needed\n    if add_bias:\n        X_new = np.hstack([np.ones((n_samples, 1)), X_new])\n    \n    return X_new\n\n    \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 500)   # input values\nmeans = np.linspace(-10, 10, 5) # 5 means: [-10, -5, 0, 5, 10]\n\n# Try different σ values\nsigmas = [0.1, 0.5, 1, 5, 10]\n\nfor sigma in sigmas:\n    plt.figure(figsize=(8,5))\n    for m in means:\n        y = np.exp(-(x - m)**2 / (2 * sigma**2))   # Gaussian basis\n        plt.plot(x, y, label=f\"µ={m}\")\n    \n    plt.title(f\"Gaussian Basis Functions (sigma={sigma})\")\n    plt.xlabel(\"Input value (x)\")\n    plt.ylabel(\"Basis output\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:45:07.436491Z","iopub.execute_input":"2025-09-05T11:45:07.436787Z","iopub.status.idle":"2025-09-05T11:45:08.753822Z","shell.execute_reply.started":"2025-09-05T11:45:07.436768Z","shell.execute_reply":"2025-09-05T11:45:08.752712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(f) Using no regularization and doing regression with this new set of basis functions, plot the\ntraining and testing error as a function of σ (when using only basis functions of a given σ). Add\nconstant lines showing the training and testing error you had obtained in part c. Explain how σ\ninfluences overfitting and the bias-variance trade-off.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# Gaussian basis transform\ndef gbt(X, sigma, n_basis=5, low=-10, high=10):\n    means = np.linspace(low, high, n_basis)\n    features = []\n    for j in range(X.shape[1]):\n        for m in means:\n            features.append(np.exp(-(X[:, j] - m) ** 2 / (2 * sigma**2)))\n    return np.array(features).T\n\nbaseline = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=5000)\nbaseline.fit(X_train, y_train)\nbaseline_train_loss = log_loss(y_train, baseline.predict_proba(X_train))\nbaseline_test_loss  = log_loss(y_test, baseline.predict_proba(X_test))\n\n# test different σ values\nsigmas = [0.1, 0.5, 1, 5, 10]\ntrain_losses, test_losses = [], []\n\nfor sigma in sigmas:\n    X_train_phi = gbt(X_train, sigma)\n    X_test_phi  = gbt(X_test, sigma)\n\n    model = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=5000)\n    model.fit(X_train_phi, y_train)\n\n    train_losses.append(log_loss(y_train, model.predict_proba(X_train_phi)))\n    test_losses.append(log_loss(y_test, model.predict_proba(X_test_phi)))\n\n# plot\nplt.plot(sigmas, train_losses, marker=\"o\", label=\"Train Loss\")\nplt.plot(sigmas, test_losses, marker=\"s\", label=\"Test Loss\")\nplt.axhline(baseline_train_loss, color=\"blue\", linestyle=\"--\", label=\"Baseline Train Loss\")\nplt.axhline(baseline_test_loss, color=\"orange\", linestyle=\"--\", label=\"Baseline Test Loss\")\nplt.xlabel(\"σ (Gaussian width)\")\nplt.ylabel(\"Log Loss\")\nplt.title(\"Training vs Testing Error as a Function of σ (No Regularization)\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:51:20.192057Z","iopub.execute_input":"2025-09-05T11:51:20.192439Z","iopub.status.idle":"2025-09-05T11:52:09.223401Z","shell.execute_reply.started":"2025-09-05T11:51:20.192415Z","shell.execute_reply":"2025-09-05T11:52:09.222341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"g:Add in all the basis function and perform regularized regression with the regularization\nparameter λ ∈ {0, 0.1, 1, 10, 100, 1000, 10000}. Plot on one graph the average cross- entropy\nerror for the training data and the testing data, as a function of λ (you should use a log scale\nfor λ). Plot on another graph the L2 norm of the weight vector you obtain. Plot on a different\ngraph the L2 norm of the weights for the set of basis functions corresponding to each value\nof σ, as a function of λ (this will be a graph with 5 lines on it). Explain briefly the results.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# Gaussian basis transform\ndef gbt(X, sigma, n_basis=5, low=-10, high=10):\n    means = np.linspace(low, high, n_basis)\n    features = []\n    for j in range(X.shape[1]):\n        for m in means:\n            features.append(np.exp(-(X[:, j] - m) ** 2 / (2 * sigma**2)))\n    return np.array(features).T\n\n# --- parameters ---\nsigmas = [0.1, 0.5, 1, 5, 10]\nlambdas = [0, 0.1, 1, 10, 100, 1000, 10000]\n\n# Combine all basis functions for all sigmas\ndef build_full_features(X):\n    features = []\n    for sigma in sigmas:\n        features.append(gbt(X, sigma))\n    return np.hstack(features)\n\nX_train_full = build_full_features(X_train)\nX_test_full  = build_full_features(X_test)\n\n# Store results\ntrain_losses, test_losses = [], []\nweight_norms = []\nsigma_weight_norms = {sigma: [] for sigma in sigmas}\n\nfor lam in lambdas:\n    if lam == 0:\n        model = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=5000)\n    else:\n        model = LogisticRegression(penalty=\"l2\", C=1/lam, solver=\"lbfgs\", max_iter=5000)\n    \n    model.fit(X_train_full, y_train)\n\n    # Losses\n    train_losses.append(log_loss(y_train, model.predict_proba(X_train_full)))\n    test_losses.append(log_loss(y_test, model.predict_proba(X_test_full)))\n\n    # L2 norm of all weights\n    W = model.coef_.flatten()\n    weight_norms.append(np.linalg.norm(W))\n\n    # Split weights per σ block\n    block_size = X_train.shape[1] * 5  # 5 basis per feature\n    for i, sigma in enumerate(sigmas):\n        start = i * block_size\n        end = (i+1) * block_size\n        sigma_weight_norms[sigma].append(np.linalg.norm(W[start:end]))\n\n# Train/Test error vs lambda\nplt.figure(figsize=(8,6))\nplt.plot(lambdas, train_losses, marker=\"o\", label=\"Train Loss\")\nplt.plot(lambdas, test_losses, marker=\"s\", label=\"Test Loss\")\nplt.xscale(\"log\")\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"Cross-Entropy Loss\")\nplt.title(\"Train/Test Error vs Regularization Strength\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n#Total L2 norm of weights vs lambda\nplt.figure(figsize=(8,6))\nplt.plot(lambdas, weight_norms, marker=\"o\")\nplt.xscale(\"log\")\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"||w||₂\")\nplt.title(\"L2 Norm of Weight Vector vs Regularization Strength\")\nplt.grid(True)\nplt.show()\n\n#L2 norm per  sigma vs lambda \nplt.figure(figsize=(8,6))\nfor sigma in sigmas:\n    plt.plot(lambdas, sigma_weight_norms[sigma], marker=\"o\", label=f\"σ={sigma}\")\nplt.xscale(\"log\")\nplt.xlabel(\"λ (log scale)\")\nplt.ylabel(\"||w||₂ for each σ\")\nplt.title(\"L2 Norm of Weights per σ vs Regularization Strength\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:53:19.352279Z","iopub.execute_input":"2025-09-05T11:53:19.352597Z","iopub.status.idle":"2025-09-05T11:53:23.069647Z","shell.execute_reply.started":"2025-09-05T11:53:19.352575Z","shell.execute_reply":"2025-09-05T11:53:23.068720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(h) Explain what you would need to do if you wanted to design a set of Gaussian basis functions\nthat capture relationships between the inputs. Explain the impact of this choice on the bias-variance trade-off. No experiments are needed (although you are welcome to explore\nthis on your own).\n(i) Suppose that instead of wanting to use a fixed set of evenly-spaced basis functions, you would\nlike to adapt the placement of these functions. Derive a learning algorithm that computes\nboth the placement of the basis function, μi and the weight vector w from data (assuming\nthat the width σ isfixed. You should still allow for L2 regularization of the weight vector.\nNote that your algorithm will need to be iterative.\n(j) Does your algorithm converge? If so, does it obtain a locally or globally optimal solution?\nExplain your answer.","metadata":{}},{"cell_type":"markdown","source":"(h) If we want Gaussian basis functions that capture relationships between inputs, \nwe cannot just use one feature at a time. Instead, we need to design \nmultivariate Gaussians that depend on two or more inputs together. \nFor example, instead of using only x1 or x2, we use functions like:\n\n   φ(x1, x2) = exp(-((x1 - μ1)^2 + (x2 - μ2)^2) / (2σ^2))\n\nThis way, the basis can capture how features interact. \n\nImpact on bias-variance:\n- Bias decreases (model becomes more flexible, can fit complex patterns).\n- Variance increases (model can overfit noise, needs regularization).\nSo, it’s a bias-variance trade-off: more complex bases → lower bias but higher variance.\n\n---------------------------------------------------------------\n\n(i) If we don’t want fixed centers (μ), we can try to learn them from data. \nWe need an algorithm that updates both:\n   - The weights (w)\n   - The centers (μ)\n\nSteps:\n1. Start with some initial μ (for example from k-means or random).\n2. Repeat until convergence:\n   a) Fix μ and find the best w (this is a convex problem, so easy).\n   b) Fix w and update μ using gradient descent:\n        μ_new = μ_old - η * (gradient of loss wrt μ)\n\nHere σ is kept fixed, and we also add L2 regularization on weights w.\n\n---------------------------------------------------------------\n\n(j) Will it converge?\n- Yes, the algorithm usually converges, but only to a **local minimum**, \n  not guaranteed to be global (because the problem is non-convex).\n- The final solution depends on initialization (starting μ values).\n- To improve results, try multiple random starts and pick the best model.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes, load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report\n\n# 1. Regression on Diabetes Dataset\n\n# Load built-in dataset\ndiabetes = load_diabetes(as_frame=True)\nX_reg, y_reg = diabetes.data, diabetes.target\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\n# Linear Regression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_pred_lin = lin_reg.predict(X_test)\n\nprint(\"Linear Regression - Regression Metrics\")\nprint(\"MSE:\", mean_squared_error(y_test, y_pred_lin))\nprint(\"R2 Score:\", r2_score(y_test, y_pred_lin))\n\n# Ridge Regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\ny_pred_ridge = ridge.predict(X_test)\n\nprint(\"\\nRidge Regression - Regression Metrics\")\nprint(\"MSE:\", mean_squared_error(y_test, y_pred_ridge))\nprint(\"R2 Score:\", r2_score(y_test, y_pred_ridge))\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_pred_lasso = lasso.predict(X_test)\n\nprint(\"\\nLasso Regression - Regression Metrics\")\nprint(\"MSE:\", mean_squared_error(y_test, y_pred_lasso))\nprint(\"R2 Score:\", r2_score(y_test, y_pred_lasso))\n\n\n# 2. Classification on Breast Cancer Dataset\n\ncancer = load_breast_cancer(as_frame=True)\nX_clf, y_clf = cancer.data, cancer.target\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n\n# Logistic Regression\nlog_reg = LogisticRegression(max_iter=5000)\nlog_reg.fit(X_train, y_train)\ny_pred_log = log_reg.predict(X_test)\n\nprint(\"\\nLogistic Regression - Classification Metrics\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_log))\n\n# Confusion matrix plot\ncm = confusion_matrix(y_test, y_pred_log)\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T11:57:06.071519Z","iopub.execute_input":"2025-09-05T11:57:06.071850Z","iopub.status.idle":"2025-09-05T11:57:16.708193Z","shell.execute_reply.started":"2025-09-05T11:57:06.071816Z","shell.execute_reply":"2025-09-05T11:57:16.707142Z"}},"outputs":[],"execution_count":null}]}