{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nclass Perceptron:\n    \n    def __init__(self, learning_rate=0.01, n_iters=1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def _heaviside_step_function(self, x):\n        \"\"\"Activation function.\"\"\"\n        # Returns 1 if x >= 0, otherwise 0\n        return np.where(x >= 0, 1, 0)\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # 1. Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Convert y to an array\n        y_ = np.array(y)\n\n        # 2. Learn from data over specified iterations\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                # Calculate the linear combination\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                \n                # Apply the step function for prediction\n                y_predicted = self._heaviside_step_function(linear_output)\n\n                # 3. Update weights and bias if there is an error\n                # Perceptron update rule: w = w + learning_rate * (y_true - y_pred) * x\n                update = self.learning_rate * (y_[idx] - y_predicted)\n                self.weights += update * x_i\n                self.bias += update\n\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self._heaviside_step_function(linear_output)\n\n\n\nif __name__ == '__main__':\n    # 1. Prepare the data\n    # AND gate inputs\n    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    # AND gate outputs\n    y_train = np.array([0, 0, 0, 1])\n\n    # 2. Create and train the Perceptron\n    perceptron = Perceptron(learning_rate=0.1, n_iters=100)\n    perceptron.fit(X_train, y_train)\n\n    print(\"Perceptron training complete!\")\n    print(f\"Final Weights: {perceptron.weights}\")\n    print(f\"Final Bias: {perceptron.bias}\")\n    \n    # 3. Test the trained Perceptron\n    print(\"\\n--- Testing the Perceptron ---\")\n    test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    \n    for test_input in test_inputs:\n        prediction = perceptron.predict(test_input)\n        print(f\"Input: {test_input} -> Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:18:14.316962Z","iopub.execute_input":"2025-10-17T04:18:14.317310Z","iopub.status.idle":"2025-10-17T04:18:14.353591Z","shell.execute_reply.started":"2025-10-17T04:18:14.317278Z","shell.execute_reply":"2025-10-17T04:18:14.352223Z"}},"outputs":[{"name":"stdout","text":"Perceptron training complete!\nFinal Weights: [0.2 0.1]\nFinal Bias: -0.20000000000000004\n\n--- Testing the Perceptron ---\nInput: [0 0] -> Prediction: 0\nInput: [0 1] -> Prediction: 0\nInput: [1 0] -> Prediction: 0\nInput: [1 1] -> Prediction: 1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#2\nimport numpy as np\n\nclass Perceptron:\n    \"\"\"A simple Perceptron classifier.\"\"\"\n\n    def __init__(self, learning_rate=0.1, n_iters=100):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def _step_function(self, x):\n        \"\"\"Heaviside step function.\"\"\"\n        return np.where(x >= 0, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\"Train the Perceptron.\"\"\"\n        n_samples, n_features = X.shape\n\n        # 1. Initialize weights and bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        y_ = np.array(y)\n\n        # 2. Learn from data\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                # Calculate the linear combination\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                \n                # Apply activation function for prediction\n                y_predicted = self._step_function(linear_output)\n\n                # 3. Update weights and bias based on error\n                # Perceptron update rule: w = w + lr * (y_true - y_pred) * x\n                update = self.learning_rate * (y_[idx] - y_predicted)\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self._step_function(linear_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:54:24.969198Z","iopub.execute_input":"2025-10-17T04:54:24.969552Z","iopub.status.idle":"2025-10-17T04:54:24.978691Z","shell.execute_reply.started":"2025-10-17T04:54:24.969519Z","shell.execute_reply":"2025-10-17T04:54:24.977412Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Data for AND Gate\nX_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\n\n#Train Perceptron for AND\np_and = Perceptron(learning_rate=0.1, n_iters=20)\np_and.fit(X_and, y_and)\n\n# Test AND Gate\nprint(\"--- AND Gate Results ---\")\nprint(f\"Final Weights: {p_and.weights}\")\nprint(f\"Final Bias: {p_and.bias}\\n\")\n\nfor inputs in X_and:\n    prediction = p_and.predict(inputs)\n    print(f\"Input: {inputs} -> Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T05:10:33.574735Z","iopub.execute_input":"2025-10-17T05:10:33.575029Z","iopub.status.idle":"2025-10-17T05:10:33.591938Z","shell.execute_reply.started":"2025-10-17T05:10:33.575009Z","shell.execute_reply":"2025-10-17T05:10:33.590675Z"}},"outputs":[{"name":"stdout","text":"--- AND Gate Results ---\nFinal Weights: [0.2 0.1]\nFinal Bias: -0.20000000000000004\n\nInput: [0 0] -> Prediction: 0\nInput: [0 1] -> Prediction: 0\nInput: [1 0] -> Prediction: 0\nInput: [1 1] -> Prediction: 1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as npfrom sklearn.neural_network import MLPClassifier\n\n# --- 1. Define the non-linear data (XOR Gate) ---\n# Inputs\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# Targets\ny = np.array([0, 1, 1, 0])\n\n\ndef train_and_test_mlp(num_neurons):\n    \"\"\"\n    Trains and tests an MLP with a specific number of hidden neurons.\n    \n    Parameters:\n    num_neurons (int): The number of neurons in the single hidden layer.\n    \"\"\"\n    print(f\"\\n--- Testing MLP with {num_neurons} hidden neuron(s) ---\")\n\n    # --- 2. Create the MLP Model ---\n    # We define one hidden layer with 'num_neurons'.\n    # hidden_layer_sizes is a tuple; (num_neurons,) means one layer.\n    # activation='relu' is a common choice. 'tanh' is another.\n    # max_iter is the number of epochs.\n    # random_state ensures we get the same result each time we run the code.\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(num_neurons,),\n        activation='relu',\n        solver='adam',\n        max_iter=1000,\n        random_state=1\n    )\n\n    # --- 3. Train the Model ---\n    mlp.fit(X, y)\n\n    # --- 4. Test the Model ---\n    print(\"Model training complete.\")\n    predictions = mlp.predict(X)\n    \n    print(\"Predictions:\")\n    for i in range(len(X)):\n        print(f\"Input: {X[i]}, Target: {y[i]}, Prediction: {predictions[i]}\")\n\n    # Check accuracy\n    accuracy = mlp.score(X, y) * 100\n    print(f\"Accuracy: {accuracy:.2f}%\")\n\n\n# --- Run experiments with various numbers of neurons ---\nif __name__ == '__main__':\n    # A single neuron cannot solve XOR\n    train_and_test_mlp(1)\n\n    # Two neurons are generally enough to solve XOR\n    train_and_test_mlp(2)\n    \n    # Using more neurons also works\n    train_and_test_mlp(4)\n    \n    # Even more neurons\n    train_and_test_mlp(8)                                                                                 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T05:34:29.408186Z","iopub.execute_input":"2025-10-17T05:34:29.408672Z","iopub.status.idle":"2025-10-17T05:34:30.427425Z","shell.execute_reply.started":"2025-10-17T05:34:29.408647Z","shell.execute_reply":"2025-10-17T05:34:30.426113Z"}},"outputs":[{"name":"stdout","text":"\n--- Testing MLP with 1 hidden neuron(s) ---\nModel training complete.\nPredictions:\nInput: [0 0], Target: 0, Prediction: 0\nInput: [0 1], Target: 1, Prediction: 0\nInput: [1 0], Target: 1, Prediction: 0\nInput: [1 1], Target: 0, Prediction: 0\nAccuracy: 50.00%\n\n--- Testing MLP with 2 hidden neuron(s) ---\nModel training complete.\nPredictions:\nInput: [0 0], Target: 0, Prediction: 0\nInput: [0 1], Target: 1, Prediction: 0\nInput: [1 0], Target: 1, Prediction: 0\nInput: [1 1], Target: 0, Prediction: 0\nAccuracy: 50.00%\n\n--- Testing MLP with 4 hidden neuron(s) ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model training complete.\nPredictions:\nInput: [0 0], Target: 0, Prediction: 0\nInput: [0 1], Target: 1, Prediction: 1\nInput: [1 0], Target: 1, Prediction: 1\nInput: [1 1], Target: 0, Prediction: 0\nAccuracy: 100.00%\n\n--- Testing MLP with 8 hidden neuron(s) ---\nModel training complete.\nPredictions:\nInput: [0 0], Target: 0, Prediction: 0\nInput: [0 1], Target: 1, Prediction: 1\nInput: [1 0], Target: 1, Prediction: 1\nInput: [1 1], Target: 0, Prediction: 0\nAccuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12}]}