{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Write a program that demonstrates the advantage of ensemble learning compared to a\nsingle classifier.\nApply a Decision Tree and a Random Forest (RF) classifier on a given dataset.\nCompare their performance using evaluation metrics such as accuracy, precision, recall, and\nF1-score.\nExplore the effect of changing the number of estimators (decision trees) in Random Forest.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2,random_state=42)\nprint(\"Training a single Decision Tree...\")\ntree_classifier = DecisionTreeClassifier(random_state=42)\n\ntree_classifier.fit(X_train, y_train)\n\ntree_predictions = tree_classifier.predict(X_test)\n\ntree_accuracy = accuracy_score(y_test, tree_predictions)\nprint(f\"Single Decision Tree Accuracy: {tree_accuracy * 100:.2f}%\")\n\n\nprint(\"Training a Random Forest (an ensemble of 100 trees)...\")\nforest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\nforest_classifier.fit(X_train, y_train)\nforest_predictions = forest_classifier.predict(X_test)\nforest_accuracy = accuracy_score(y_test, forest_predictions)\nprint(f\"Random Forest (Ensemble) Accuracy: {forest_accuracy * 100:.2f}%\")\nprint(f\"forest_accuracy -> {forest_accuracy:.4f}, tree_accuracy->{tree_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:32:05.318689Z","iopub.execute_input":"2025-10-24T04:32:05.319054Z","iopub.status.idle":"2025-10-24T04:32:07.990802Z","shell.execute_reply.started":"2025-10-24T04:32:05.319031Z","shell.execute_reply":"2025-10-24T04:32:07.989638Z"}},"outputs":[{"name":"stdout","text":"Data split: 142 training samples, 36 testing samples.\n\nTraining a single Decision Tree...\nSingle Decision Tree Accuracy: 94.44%\n\nTraining a Random Forest (an ensemble of 100 trees)...\nRandom Forest (Ensemble) Accuracy: 100.00%\nforest_accuracy -> 1.0000, tree_accuracy->0.9444\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"2. Write a program that demonstrates the use of simple ensemble techniques: Max Voting,\nAverage Voting, and Weighted Average Voting (assign weights based on each model’s\nperformance).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(wine.data,wine.target, test_size=0.2,random_state=42)\n\ndt = DecisionTreeClassifier(random_state=42)\nrf = RandomForestClassifier(n_estimators=5, random_state=42) # A small forest\nlr = LogisticRegression(random_state=42, max_iter=10000)\n\ndt.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nlr.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\ny_pred_rf = rf.predict(X_test)\ny_pred_lr = lr.predict(X_test)\n\npredictions = np.array([y_pred_dt, y_pred_rf, y_pred_lr]).T\n\ny_pred_max = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)\n\n\ny_prob_dt = dt.predict_proba(X_test)\ny_prob_rf = rf.predict_proba(X_test)\ny_prob_lr = lr.predict_proba(X_test)\n\n\ny_prob_avg = (y_prob_dt + y_prob_rf + y_prob_lr) / 3.0\n\ny_pred_avg = np.argmax(y_prob_avg, axis=1)\n\nacc_dt = accuracy_score(y_test, y_pred_dt)\nacc_rf = accuracy_score(y_test, y_pred_rf)\nacc_lr = accuracy_score(y_test, y_pred_lr)\n\nweights = np.array([acc_dt, acc_rf, acc_lr])\n\nnormalized_weights = weights / weights.sum()\n\ny_prob_weighted = (y_prob_dt * normalized_weights[0] + y_prob_rf * normalized_weights[1] + y_prob_lr * normalized_weights[2])\n\ny_pred_weighted = np.argmax(y_prob_weighted, axis=1)\n\ndef evaluate_model(name, y_true, y_pred):\n    print(f\"\\n--- {name} ---\")\n    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n    print(f\"Recall   : {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n    print(f\"F1-score : {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n\nprint(\"\\n Model Evaluations \")\n\n# Evaluate all our models\nevaluate_model(\"Decision Tree (Single)\", y_test, y_pred_dt)\nevaluate_model(\"Random Forest (5 Trees)\", y_test, y_pred_rf)\nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\nevaluate_model(\"Ensemble: Max Voting\", y_test, y_pred_max)\nevaluate_model(\"Ensemble: Average Voting\", y_test, y_pred_avg)\nevaluate_model(\"Ensemble: Weighted Average\", y_test, y_pred_weighted)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:48:17.823055Z","iopub.execute_input":"2025-10-24T04:48:17.823452Z","iopub.status.idle":"2025-10-24T04:48:18.546406Z","shell.execute_reply.started":"2025-10-24T04:48:17.823423Z","shell.execute_reply":"2025-10-24T04:48:18.545497Z"}},"outputs":[{"name":"stdout","text":"\n Model Evaluations \n\n--- Decision Tree (Single) ---\nAccuracy : 0.9444\nPrecision: 0.9463\nRecall   : 0.9444\nF1-score : 0.9440\n\n--- Random Forest (5 Trees) ---\nAccuracy : 0.8889\nPrecision: 0.8889\nRecall   : 0.8889\nF1-score : 0.8889\n\n--- Logistic Regression ---\nAccuracy : 1.0000\nPrecision: 1.0000\nRecall   : 1.0000\nF1-score : 1.0000\n\n--- Ensemble: Max Voting ---\nAccuracy : 0.9722\nPrecision: 0.9741\nRecall   : 0.9722\nF1-score : 0.9718\n\n--- Ensemble: Average Voting ---\nAccuracy : 0.9722\nPrecision: 0.9741\nRecall   : 0.9722\nF1-score : 0.9722\n\n--- Ensemble: Weighted Average ---\nAccuracy : 0.9722\nPrecision: 0.9741\nRecall   : 0.9722\nF1-score : 0.9722\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"3.Write a program to show the difference between Hard Voting and Soft Voting classifiers in\nensemble learning using multiple base learners (e.g., Decision Tree, Logistic Regression, and\nKNN)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\n\nwine = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(wine.data,wine.target, test_size=0.3,random_state=42)\n\nprint(f\"Using {len(X_train)} samples for training and {len(X_test)} for testing.\\n\")\n\nmodel_1 = LogisticRegression(random_state=42, max_iter=10000)\nmodel_2 = DecisionTreeClassifier(random_state=42)\nmodel_3 = KNeighborsClassifier(n_neighbors=5)\n\nhard_voting_clf = VotingClassifier(\n    estimators=[('lr', model_1), ('dt', model_2), ('knn', model_3)],voting='hard')\n\nprint(\"Training the Hard Voting classifier...\")\nhard_voting_clf.fit(X_train, y_train)\ny_pred_hard = hard_voting_clf.predict(X_test)\nacc_hard = accuracy_score(y_test, y_pred_hard)\n\nprint(f\"Hard Voting Accuracy: {acc_hard * 100:.2f}%\")\n\n\nsoft_voting_clf = VotingClassifier(\n    estimators=[('lr', model_1),('dt', model_2), ('knn', model_3)],voting='soft')\n\nprint(\"\\nTraining the Soft Voting classifier...\")\nsoft_voting_clf.fit(X_train, y_train)\ny_pred_soft = soft_voting_clf.predict(X_test)\nacc_soft = accuracy_score(y_test, y_pred_soft)\n\nprint(f\"Soft Voting Accuracy: {acc_soft * 100:.2f}%\")\n\nprint(\"\\n--- Comparison ---\")\nprint(f\"Hard Voting (Max Votes) : {acc_hard * 100:.2f}%\")\nprint(f\"Soft Voting (Avg Probs): {acc_soft * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:56:53.547974Z","iopub.execute_input":"2025-10-24T04:56:53.548413Z","iopub.status.idle":"2025-10-24T04:56:54.839298Z","shell.execute_reply.started":"2025-10-24T04:56:53.548381Z","shell.execute_reply":"2025-10-24T04:56:54.837676Z"}},"outputs":[{"name":"stdout","text":"Hard Voting Accuracy: 100.00%\nSoft Voting Accuracy: 98.15%\n\n--- Comparison ---\nHard Voting (Max Votes) : 100.00%\nSoft Voting (Avg Probs): 98.15%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"4. Write a program using the RandomForestRegressor model to make predictions on a\nsuitable regression dataset.\nEnable and observe the oob_score (Out-of-Bag score) parameter.\nInterpret the results and explain its significance.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_diabetes  # <-- CHANGED: Using the Diabetes dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\ndiabetes = load_diabetes() \nX, y = diabetes.data, diabetes.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nrf_model = RandomForestRegressor(n_estimators=100,random_state=42,oob_score=True)\nrf_model.fit(X_train, y_train)\noob_score = rf_model.oob_score_\nprint(f\"\\nModel's Out-of-Bag (OOB) Score: {oob_score:.4f}\")\n\ny_pred = rf_model.predict(X_test)\n\ntest_score = r2_score(y_test, y_pred)\nprint(f\"Model's Test Set Score        : {test_score:.4f}\")\nprint(\"\\n--- What does this mean? ---\")\nprint(f\"\"\"\nWhat is the OOB Score?\n- A Random Forest is a \"forest\" of many decision trees.\n- Each tree is trained on a random *sample* of the training data.\n- This means each tree *misses* about 1/3 of the training data. This \"left out\" data is called its \"Out-of-Bag\" (OOB) data.\n- The OOB Score is a \"test\" performed *during training*. For each data point, the model uses all the trees that *did not* see that point to make a prediction.\n- It then compares all these \"OOB predictions\" to the true answers.\n\nSignificance:\n- The OOB Score is a reliable estimate of how well the model will perform on *new, unseen data*.\n- It's like a \"free\" test score you get without needing a separate test set.\n- **Notice how our OOB Score ({oob_score:.4f}) is very close to our Test Set Score ({test_score:.4f}).**\n- This closeness gives us confidence that our model is good at generalizing and isn't just \"memorizing\" the training data.\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:06:20.031831Z","iopub.execute_input":"2025-10-24T05:06:20.032660Z","iopub.status.idle":"2025-10-24T05:06:20.321472Z","shell.execute_reply.started":"2025-10-24T05:06:20.032623Z","shell.execute_reply":"2025-10-24T05:06:20.320476Z"}},"outputs":[{"name":"stdout","text":"\nModel's Out-of-Bag (OOB) Score: 0.4294\nModel's Test Set Score        : 0.4703\n\n--- What does this mean? ---\n\nWhat is the OOB Score?\n- A Random Forest is a \"forest\" of many decision trees.\n- Each tree is trained on a random *sample* of the training data.\n- This means each tree *misses* about 1/3 of the training data. This \"left out\" data is called its \"Out-of-Bag\" (OOB) data.\n- The OOB Score is a \"test\" performed *during training*. For each data point, the model uses all the trees that *did not* see that point to make a prediction.\n- It then compares all these \"OOB predictions\" to the true answers.\n\nSignificance:\n- The OOB Score is a reliable estimate of how well the model will perform on *new, unseen data*.\n- It's like a \"free\" test score you get without needing a separate test set.\n- **Notice how our OOB Score (0.4294) is very close to our Test Set Score (0.4703).**\n- This closeness gives us confidence that our model is good at generalizing and isn't just \"memorizing\" the training data.\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"5. Write a program to explore different Boosting techniques using suitable datasets:\nAdaptive Boosting (AdaBoost) – binary classification.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nweak_learner = DecisionTreeClassifier(max_depth=1, random_state=42)\nweak_learner.fit(X_train, y_train)\ny_pred_weak = weak_learner.predict(X_test)\n\nacc_weak = accuracy_score(y_test, y_pred_weak)\nprint(f\"Accuracy of one weak tree: {acc_weak * 100:.2f}%\")\nadaboost_model = AdaBoostClassifier(estimator=weak_learner,n_estimators=50,random_state=42)\n\nadaboost_model.fit(X_train, y_train)\ny_pred_adaboost = adaboost_model.predict(X_test)\n\nacc_adaboost = accuracy_score(y_test, y_pred_adaboost)\nprint(f\"Accuracy of AdaBoost model: {acc_adaboost * 100:.2f}%\")\n\nprint(\"\\n--- What does this mean? ---\")\nprint(f\"\"\"\n1. The single, weak tree (stump) was only {acc_weak * 100:.2f}% accurate.\n2. The AdaBoost model, which combined 50 of these stumps, achieved {acc_adaboost * 100:.2f}% accuracy.\n\nSignificance:\nAdaBoost (Adaptive Boosting) creates a powerful, accurate model\nby \"boosting\" the performance of many simple, weak ones.\n\nIt trains them one after another, and each new tree focuses on\nfixing the mistakes made by the previous trees. This sequential\nlearning process is what makes boosting models so effective.\n\"\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T05:10:51.860922Z","iopub.execute_input":"2025-10-24T05:10:51.861340Z","iopub.status.idle":"2025-10-24T05:10:52.040145Z","shell.execute_reply.started":"2025-10-24T05:10:51.861311Z","shell.execute_reply":"2025-10-24T05:10:52.039106Z"}},"outputs":[{"name":"stdout","text":"Accuracy of one weak tree: 89.47%\nAccuracy of AdaBoost model: 97.66%\n\n--- What does this mean? ---\n\n1. The single, weak tree (stump) was only 89.47% accurate.\n2. The AdaBoost model, which combined 50 of these stumps, achieved 97.66% accuracy.\n\nSignificance:\nAdaBoost (Adaptive Boosting) creates a powerful, accurate model\nby \"boosting\" the performance of many simple, weak ones.\n\nIt trains them one after another, and each new tree focuses on\nfixing the mistakes made by the previous trees. This sequential\nlearning process is what makes boosting models so effective.\n\n","output_type":"stream"}],"execution_count":28}]}